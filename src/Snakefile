import pathlib
from samples import samples

genome_by_tissue = {
    "liver": {
        "dir": "/project/itmatlab/index/STAR-2.7.10b_indexes/GRCm38.ensemblv102",
        "gtf": "/project/itmatlab/index/STAR-2.7.6a_indexes/GRCm38.ensemblv102/Mus_musculus.GRCm38.102.gtf",
        "BSgenome": "BSgenome.Mmusculus.UCSC.mm10",
        "name": "Mus_musculus.GRCm38.102"
    },
    "testis": {
        "dir": "/project/itmatlab/index/STAR-2.7.10b_indexes/GRCm38.ensemblv102",
        "gtf": "/project/itmatlab/index/STAR-2.7.6a_indexes/GRCm38.ensemblv102/Mus_musculus.GRCm38.102.gtf",
        "BSgenome": "BSgenome.Mmusculus.UCSC.mm10",
        "name": "Mus_musculus.GRCm38.102"
    },
    "UHR": {
        "dir": "/project/itmatlab/index/STAR-2.7.10b_indexes/GRCh38.ensemblv109",
        "gtf": "/project/itmatlab/index/STAR-2.7.10b_indexes/GRCh38.ensemblv109/Homo_sapiens.GRCh38.109.gtf",
        "BSgenome": "BSgenome.Hsapiens.UCSC.hg38",
        "name": "GRCh38.ensemblv109",
    }
}
gtf_by_name = {x['name']:x['gtf'] for x in genome_by_tissue.values()}

samples_by_id = {sample['ID']: sample for sample in samples}
sample_ids = [s['ID'] for s in samples]
tissues = list(set(sample['tissue'] for sample in samples))
samples_by_tissue = {tissue: [s['ID'] for s in samples if s['tissue'] == tissue] for tissue in tissues}

RCLONE_REMOTE = "aws_public" # Must first run `rclone config` and set up a remote with this name for uploading trackhubs to
BUCKET_NAME = "itmat.data" # Bucket to upload to with rclone for trackhubs
BUCKET_DIR = "PCR_cycle_count"

def num_reads(sample_id):
   # Returns 1 if single-end and 2 if paired-end
   fastq_dir = str(checkpoints.download_srx.get(sample_id = sample_id).output)
   fastq = list(pathlib.Path(fastq_dir).glob("*.fastq"))[0].name
   return 2 if "_" in fastq else 1

def processed_fastq_files(sample_id):
  if num_reads(sample_id) > 1:
    return [f"data/{wildcards.sample_id}/processed_fastqs/R1.fastq", f"data/{wildcards.sample_id}/processed_fastqs/R2.fastq"]
  else:
    return ["data/{sample_id/processed_fastqs/R1.fastq"]

def can_alpine(sample_id):
  # Alpine requires paired-end data
  return num_reads(sample_id) > 1

def get_readlen(sample_id):
  fastq_dir = pathlib.Path(str(checkpoints.download_srx.get(sample_id = sample_id).output))
  fastq_file = list(fastq_dir.glob("*.fastq"))[0]
  with open(fastq_file) as f:
    f.readline()
    read = f.readline() 
    return len(read) - 1
  #with open(f"data/{sample_id}/bam/Log.final.out") as f:
  #  for line in f:
  #    if "Average input read length" in line:
  #      star_read_len = int(line.split("|")[1].strip())
  #      n = num_reads(sample_id)
  #      return star_read_len // n # If paired-end, STAR reports both reads summed together
  #  raise Exception(f"Could not identify a read length for {sample_id}")

    
rule all:
    input:
        #expand("data/{sample_id}/bam/Aligned.sortedByCoord.out.bam", sample_id = sample_ids),
        #expand("results/{sample_id}.coverage.fwd.bw", sample_id = sample_ids),
        #expand("results/{sample_id}.coverage.rev.bw", sample_id = sample_ids),
        #expand("results/{sample_id}.coverage.unstranded.bw", sample_id = sample_ids),
        expand("data/{sample_id}/umi_groups/grouped.Aligned.sortedByCoord.out.bam", sample_id = [s for s in sample_ids if 'UMI' in samples_by_id[s]]),
        #"data/liver/high_expressed_single_isoform_genes.txt",
        #"results/url.txt",
        #"results/alpine/liver/",
        "results/alpine_fit_plots/liver/",
        "results/alpine_fit_plots/UHR/",
        #"results/alpine_rnafold_fits/liver/",
        #lambda wildcards: expand("data/{sample_id}/alpine_rnafold.model.rda", sample_id = [id for id in samples_by_tissue['liver'] if can_alpine(id)]),
        "results/testis/pcr_dupe_rate_coverage_plot/",

rule generate_sif:
    input:
        "containers/{container}.def"
    output:
        "images/{container}.sif"
    resources:
        mem_mb = 6_000
    shell:
        "apptainer build {output} {input}"

rule fetch_srr_ids:
    output:
        "data/{sample_id}/SRR.txt"
    shell:
        "esearch -db sra -query {wildcards.sample_id} | efetch -format docsum |xtract -pattern DocumentSummary -element Run@acc > {output}"

checkpoint download_srx:
    input:
        "data/{sample_id}/SRR.txt"
    output:
        directory("data/{sample_id}/fastq")
    resources:
      ncbi_download = 1
    shell:
        # TODO: support multiple SRRs for a single SRX?
        "fasterq-dump -A `cat {input}` -O {output} --split-3"

rule process_fastqs:
    input:
        read_dir = "data/{sample_id}/fastq"
    output:
        read_dir = directory("data/{sample_id}/processed_fastqs/")
    run:
        sample = samples_by_id[wildcards.sample_id]
        read_dir = pathlib.Path(input.read_dir)
        out_dir = pathlib.Path(output.read_dir)
        out_dir.mkdir()
        if num_reads(wildcards.sample_id) > 1:
            R1 = sorted([str(x) for x in read_dir.glob("*_1.fastq")])
            R2 = sorted([str(x) for x in read_dir.glob("*_2.fastq")])
            R1_out = out_dir / "R1.fastq"
            R2_out = out_dir / "R2.fastq"

            # concatenate
            concat_R1 = out_dir / "R1.concat.fastq"
            concat_R2 = out_dir / "R2.concat.fastq"
            shell(f"cat {' '.join(R1)} > {concat_R1}")
            shell(f"cat {' '.join(R2)} > {concat_R2}")

            # Extract UMIs
            if 'UMI' in sample:
              # Extract UMIs from sequence
              log = out_dir / "extract.log"
              R1_code = sample['UMI']['read1']
              R2_code = sample['UMI']['read2']
              shell(f"umi_tools extract --extract-method=string --bc-pattern {R1_code} --bc-pattern2 {R2_code} --read2-in {concat_R2} --read2-out {R2_out} -L {log} < {concat_R1} > {R1_out}")
              concat_R1.unlink()
              concat_R2.unlink()
            else:
              # Nothing to do
              concat_R1.rename(R1_out)
              concat_R2.rename(R2_out)
        else:
            R1 = sorted([str(x) for x in read_dir.glob("*.fastq")])
            R1_out = out_dir / "R1.fastq"

            # concatenate
            concat_R1 = out_dir / "R1.concat.fastq"
            shell(f"cat {' '.join(R1)} > {concat_R1}")

            # Extract UMIs
            if 'UMI' in sample:
              # Extract UMIs from sequence
              log = out_dir / "extract.log"
              R1_code = sample['UMI']['read1']
              shell(f"umi_tools extract --extract-method=string --bc-pattern {R1_code} -L {log} < {concat_R1} > {R1_out}")
              concat_R1.unlink()
            else:
              # Nothing to do
              concat_R1.rename(R1_out)

checkpoint make_bam:
    input:
        genome = lambda wildcards: genome_by_tissue[samples_by_id[wildcards.sample_id]['tissue']]['dir'],
        reads = "data/{sample_id}/processed_fastqs",
    output:
        bam = protected("data/{sample_id}/bam/Aligned.sortedByCoord.out.bam"),
        quants = protected("data/{sample_id}/bam/ReadsPerGene.out.tab"),
        log = protected("data/{sample_id}/bam/Log.final.out"),
    params:
        outdir = "data/{sample_id}/bam/",
    threads: 6
    resources:
        mem_mb = 36_000,
    run:
        nr = num_reads(wildcards.sample_id)
        cmd = f"STAR --runThreadN {threads} --genomeDir {input.genome} --readFilesIn `find {input.reads} -name *fastq` --outFileNamePrefix {params.outdir} --outSAMtype BAM SortedByCoordinate --quantMode GeneCounts"
        shell(cmd)

rule make_coverage:
    input:
        bam = "data/{sample_id}/bam/Aligned.sortedByCoord.out.bam"
    output:
        "data/{sample_id}/coverage.{strand}.cov",
    params:
        strand = lambda wildcards: {"fwd": "-strand +", "rev": "-strand -", "unstranded": ""}[wildcards.strand],
        name = lambda wildcards: samples_by_id[wildcards.sample_id]['name'],
    resources:
        mem_mb = 24_000,
    shell:
        "bedtools genomecov -ibam {input} -bga -split {params.strand} -trackline  -trackopts 'name=\"{params.name} {wildcards.strand}\"' | bedtools sort > {output}"

rule make_bigwig:
    input:
      cov = "data/{sample_id}/coverage.{strand}.cov",
      sizes = lambda wildcards: f"{genome_by_tissue[samples_by_id[wildcards.sample_id]['tissue']]['dir']}/chrNameLength.txt"
    output:
      "results/{sample_id}.coverage.{strand}.bw"
    resources:
      mem_mb = 24_000,
    shell:
      "bedGraphToBigWig {input.cov} {input.sizes} {output}"

rule upload_coverage:
  input:
      expand("results/{sample_id}.coverage.{strand}.bw", sample_id = sample_ids, strand = ["fwd", "rev"])
  output:
    url = "results/url.txt"
  run:
    shell(f"rclone copy results {RCLONE_REMOTE}:{BUCKET_NAME}/{BUCKET_DIR}/")
    pathlib.Path(output.url).write_text(f"https://{BUCKET_NAME}.s3.amazonaws.com/{BUCKET_DIR}/\n")

rule index_BAMS:
    input:
        bam = "{file}.sortedByCoord.out.bam"
    output:
        bam = "{file}.sortedByCoord.out.bam.bai"
    resources:
        mem_mb = 4_000
    shell:
        "samtools index {input}"

rule make_ensembldb:
    input:
        gtf = lambda wildcards: gtf_by_name[wildcards.genome_name],
        sif = "images/alpine.sif",
    output:
        sqlite = "data/{genome_name}.gtf.sqlite"
    resources:
        mem_mb = 12_000
    container:
        "images/alpine.sif"
    script:
        "scripts/make_ensembldb.R"

rule select_alpine_transcripts:
    input:
        quants = lambda wildcards: expand("data/{sample_id}/bam/ReadsPerGene.out.tab", sample_id=samples_by_tissue[wildcards.tissue]),
        ensdb = lambda wildcards: f"data/{genome_by_tissue[wildcards.tissue]['name']}.gtf.sqlite",
    output:
        outfile = "data/{tissue}/high_expressed_single_isoform_genes.txt"
    script:
        "scripts/select_alpine_transcripts.py"

rule run_alpine:
    input:
        bam = "data/{sample_id}/bam/Aligned.sortedByCoord.out.bam",
        bai = "data/{sample_id}/bam/Aligned.sortedByCoord.out.bam.bai",
        ensdb = lambda wildcards: f"data/{genome_by_tissue[samples_by_id[wildcards.sample_id]['tissue']]['name']}.gtf.sqlite",
        transcripts = lambda wildcards: f"data/{samples_by_id[wildcards.sample_id]['tissue']}/high_expressed_single_isoform_genes.txt",
        dummy = "data/{sample_id}/fastq",
        sif = "images/alpine.sif",
    output:
        modelfile = protected("data/{sample_id}/alpine.model.rda"),
    params:
        readlength =  lambda wildcards: get_readlen(wildcards.sample_id),
        BSgenome = lambda wildcards: genome_by_tissue[samples_by_id[wildcards.sample_id]['tissue']]['BSgenome'],
    resources:
        mem_mb = 48_000,
    container:
        "images/alpine.sif"
    script:
        "scripts/run_alpine.R"

rule gen_alpine_rnafold_fragtypes:
    input:
        ensdb = lambda wildcards: f"data/{genome_by_tissue[wildcards.tissue]['name']}.gtf.sqlite",
        transcripts = "data/{tissue}/high_expressed_single_isoform_genes.txt",
        sif = "images/alpine.sif",
    output:
        fragtypes = protected("data/{tissue}/alpine_rnafold.fragtypes.rda"),
    params:
        readlength = 150, # Doesn't actually matter
        BSgenome = lambda wildcards: genome_by_tissue[wildcards.tissue]['BSgenome'],
    resources:
        mem_mb = 32_000,
    container:
        "images/alpine.sif"
    script:
        "scripts/gen_alpine_rnafold_fragtypes.R"

rule run_alpine_rnafold:
    input:
        bam = "data/{sample_id}/bam/Aligned.sortedByCoord.out.bam",
        bai = "data/{sample_id}/bam/Aligned.sortedByCoord.out.bam.bai",
        ensdb = lambda wildcards: f"data/{genome_by_tissue[wildcards.tissue]['name']}.gtf.sqlite",
        transcripts = lambda wildcards: f"data/{samples_by_id[wildcards.sample_id]['tissue']}/high_expressed_single_isoform_genes.txt",
        fragtypes = lambda wildcards: f"data/{samples_by_id[wildcards.sample_id]['tissue']}/alpine_rnafold.fragtypes.rda",
        dummy = "data/{sample_id}/fastq",
        sif = "images/alpine.sif",
    output:
        modelfile = protected("data/{sample_id}/alpine_rnafold.model.rda"),
    params:
        readlength = lambda wildcards: get_readlen(wildcards.sample_id),
        BSgenome = lambda wildcards: genome_by_tissue[samples_by_id[wildcards.sample_id]['tissue']]['BSgenome'],
    resources:
        mem_mb = 48_000,
    container:
        "images/alpine.sif"
    script:
        "scripts/run_alpine_rnafold.R"

rule compare_alpine:
    input:
        models = lambda wildcards: expand("data/{sample_id}/alpine.model.rda", sample_id = [s for s in samples_by_tissue[wildcards.tissue] if can_alpine(s)]),
        ensdb = lambda wildcards: f"data/{genome_by_tissue[wildcards.tissue]['name']}.gtf.sqlite",
        transcripts = "data/{tissue}/high_expressed_single_isoform_genes.txt",
        sif = "images/alpine.sif",
    output:
        outdir = directory("results/alpine/{tissue}/"),
    resources:
        mem_mb = 6_000,
    container:
        "images/alpine.sif"
    script:
        "scripts/compare_alpine.R"

rule compute_alpine_fits:
    input:
        bam = ["data/{sample_id}/bam/Aligned.sortedByCoord.out.bam"],
        models = ["data/{sample_id}/alpine.model.rda"],
        ensdb = lambda wildcards: f"data/{genome_by_tissue[wildcards.tissue]['name']}.gtf.sqlite",
        transcripts = "data/{tissue}/high_expressed_single_isoform_genes.txt",
        sif = "images/alpine.sif",
    output:
        outfile = "results/alpine_fits/{tissue}/{sample_id}.coverage_table.txt",
    params:
        BSgenome = lambda wildcards: genome_by_tissue[wildcards.tissue]['BSgenome'],
    resources:
        mem_mb = 46_000,
    container:
        "images/alpine.sif"
    script:
        "scripts/compute_alpine_fits.R"

rule plot_alpine_fits:
    input:
        cov_tables = lambda wildcards: expand("results/alpine_fits/{{tissue}}/{sample_id}.coverage_table.txt", sample_id = [s for s in samples_by_tissue[wildcards.tissue] if can_alpine(s)]),
        models = lambda wildcards: expand("data/{sample_id}/alpine.model.rda", sample_id = [s for s in samples_by_tissue[wildcards.tissue] if can_alpine(s)]),
        ensdb = lambda wildcards: f"data/{genome_by_tissue[wildcards.tissue]['name']}.gtf.sqlite",
        transcripts = "data/{tissue}/high_expressed_single_isoform_genes.txt",
        sample_info = "results/sample_info.txt",
        sif = "images/alpine.sif",
    output:
        outdir = directory("results/alpine_fit_plots/{tissue}/"),
    resources:
        mem_mb = 24_000,
    container:
        "images/alpine.sif"
    script:
        "scripts/plot_alpine_fits.R"

rule plot_alpine_rnafold_fits:
    input:
        bam = lambda wildcards: expand("data/{sample_id}/bam/Aligned.sortedByCoord.out.bam", sample_id = [s for s in samples_by_tissue[wildcards.tissue] if can_alpine(s)]),
        models = lambda wildcards: expand("data/{sample_id}/alpine_rnafold.model.rda", sample_id = [s for s in samples_by_tissue[wildcards.tissue] if can_alpine(s)]),
        ensdb = lambda wildcards: f"data/{genome_by_tissue[wildcards.tissue]['name']}.gtf.sqlite",
        transcripts = "data/{tissue}/high_expressed_single_isoform_genes.txt",
        fragtypes = "data/{tissue}/alpine_rnafold.fragtypes.rda",
        sif = "images/alpine.sif",
    output:
        outdir = directory("results/alpine_rnafold_fits/{tissue}/"),
    params:
        BSgenome = lambda wildcards: genome_by_tissue[wildcards.tissue]['BSgenome'],
    resources:
        mem_mb = 46_000,
    container:
        "images/alpine.sif"
    script:
        "scripts/plot_alpine_rnafold_fits.R"

rule pcr_copy_count_groups:
    input:
        bam = "data/{sample_id}/bam/Aligned.sortedByCoord.out.bam",
        bai = "data/{sample_id}/bam/Aligned.sortedByCoord.out.bam.bai",
    output:
        bam = "data/{sample_id}/umi_groups/grouped.Aligned.sortedByCoord.out.bam",
    params:
        dir = "data/{sample_id}/umi_groups/",
    resources:
        mem_mb = 48_000
    shell:
        "mkdir -p {params.dir} && umi_tools group --output-bam --unpaired-reads discard --chimeric-pairs discard --umi-separator=_ --paired --multimapping-detection-method=NH -I {input.bam}  -S {output.bam}"

rule pcr_dupe_rate_coverage:
    input:
        bam = "data/{sample_id}/umi_groups/grouped.Aligned.sortedByCoord.out.bam",
        bai = "data/{sample_id}/umi_groups/grouped.Aligned.sortedByCoord.out.bam.bai",
        chrom_lengths = lambda wildcards: f"{genome_by_tissue[samples_by_id[wildcards.sample_id]['tissue']]['dir']}/chrNameLength.txt"
    output:
        coverage = "data/{sample_id}/pcr_dupe_rate_coverage/{sample_id}.deduped_coverage.bed",
        duped_coverage = "data/{sample_id}/pcr_dupe_rate_coverage/{sample_id}.duped_coveage.bed",
        dupe_rate = "data/{sample_id}/pcr_dupe_rate_coverage/{sample_id}.pcr_dupe_rate_coverage.bed",
    resources:
        mem_mb = 12_000
    shell:
        "./scripts/pcr_dupe_rate_coverage_plot.py --bam_file {input.bam} --chrom_lengths {input.chrom_lengths} --out_cov_file {output.coverage} --out_duped_file {output.duped_coverage} --out_dupe_rate_file {output.dupe_rate}"

rule plot_pcr_dupe_rate_coverage:
    input:
        coverage = expand("data/{sample_id}/pcr_dupe_rate_coverage/{sample_id}.deduped_coverage.bed", sample_id = [s for s in sample_ids if 'UMI' in samples_by_id[s]]),
        dupe_rate = expand("data/{sample_id}/pcr_dupe_rate_coverage/{sample_id}.pcr_dupe_rate_coverage.bed", sample_id = [s for s in sample_ids if 'UMI' in samples_by_id[s]]),
        annotation = lambda wildcards: f"data/{genome_by_tissue[wildcards.tissue]['name']}.gtf.sqlite",
        transcripts = "data/{tissue}/high_expressed_single_isoform_genes.txt",
    output:
        outdir = directory("results/{tissue}/pcr_dupe_rate_coverage_plot/")
    params:
      sample_ids = [s for s in sample_ids if 'UMI' in samples_by_id[s]],
    resources:
        mem_mb = 24_000
    script:
      "scripts/plot_pcr_dupe_rate_coverage.py"

rule sample_info:
    output:
        "results/sample_info.txt"
    run:
        import polars as pl
        pl.DataFrame(samples)\
            .select(pl.all().cast(pl.Utf8))\
            .write_csv("results/sample_info.txt", separator="\t")
